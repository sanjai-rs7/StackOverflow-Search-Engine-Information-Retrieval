{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90b7a5af-be81-47dc-b7d2-53a7eebf42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee571a32-90b1-4569-830a-e035d3e7439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_file = r\"R:\\Study\\5th Sem\\Information Retrieval\\IR PROJECT\\Questions_sample.csv\"\n",
    "answers_file = r\"R:\\Study\\5th Sem\\Information Retrieval\\IR PROJECT\\Answers_sample.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88972787-9cbd-4ff1-992d-b60435c54fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of stop words\n",
    "stop_words = set([\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and',\n",
    "    'any', 'are', 'aren\\'t', 'as', 'at', 'be', 'because', 'been', 'before', 'being',\n",
    "    'below', 'between', 'both', 'but', 'by', 'can', 'could', 'couldn\\'t', 'did',\n",
    "    'didn\\'t', 'do', 'does', 'doesn\\'t', 'doing', 'don\\'t', 'down', 'during', 'each',\n",
    "    'few', 'for', 'from', 'further', 'had', 'hadn\\'t', 'has', 'hasn\\'t', 'have',\n",
    "    'haven\\'t', 'having', 'he', 'he\\'s', 'her', 'here', 'here\\'s', 'hers', 'herself',\n",
    "    'him', 'himself', 'his', 'how', 'i', 'i\\'m', 'if', 'in', 'into', 'is', 'isn\\'t',\n",
    "    'it', 'it\\'s', 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'might', 'mightn\\'t',\n",
    "    'more', 'most', 'must', 'mustn\\'t', 'my', 'myself', 'needn\\'t', 'no', 'nor',\n",
    "    'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our',\n",
    "    'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan\\'t', 'she',\n",
    "    'she\\'s', 'should', 'should\\'ve', 'so', 'some', 'such', 't', 'than', 'that',\n",
    "    'that\\'s', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there',\n",
    "    'there\\'s', 'these', 'they', 'they\\'re', 'this', 'those', 'through', 'to', 'too',\n",
    "    'under', 'until', 'up', 've', 'very', 'was', 'wasn\\'t', 'we', 'we\\'re', 'were',\n",
    "    'weren\\'t', 'what', 'what\\'s', 'when', 'where', 'which', 'while', 'who', 'who\\'s',\n",
    "    'whom', 'why', 'will', 'with', 'won\\'t', 'would', 'wouldn\\'t', 'you', 'you\\'re',\n",
    "    'your', 'yours', 'yourself', 'yourselves'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18230fc3-33bd-4571-9fb3-52dc70479f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = {}\n",
    "tfidf_documents = {}\n",
    "question_titles = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "120d38b1-daa5-46c3-99fb-cc4821d6a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'R:\\Study\\5th Sem\\Information Retrieval\\IR PROJECT\\Saved Files\\TF-IDF\\idf.pkl', 'rb') as f:\n",
    "    idf = pickle.load(f)\n",
    "with open(r'R:\\Study\\5th Sem\\Information Retrieval\\IR PROJECT\\Saved Files\\TF-IDF\\tfidf_documents.pkl', 'rb') as f:\n",
    "    tfidf_documents = pickle.load(f)\n",
    "with open(r'R:\\Study\\5th Sem\\Information Retrieval\\IR PROJECT\\Saved Files\\TF-IDF\\question_titles.pkl', 'rb') as f:\n",
    "    question_titles = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ad8e99-7ec1-4e5c-96a0-252d885f4667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build inverted index and precompute TF-IDF\n",
    "def precompute_tfidf(questions_file):\n",
    "    global idf, tfidf_documents, question_titles\n",
    "    term_frequencies = defaultdict(lambda: defaultdict(int))  # Term frequency per document\n",
    "    df = defaultdict(int)  # Document frequency\n",
    "    N = 0  # Total number of documents\n",
    "\n",
    "    with open(questions_file, 'r', encoding='latin-1') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header row if it exists\n",
    "        for row in reader:\n",
    "            question_id = row[0]\n",
    "            title = row[5]\n",
    "            if title:  # Skip empty titles\n",
    "                title = title.lower()  # Normalize text\n",
    "                words = title.split()  # Tokenize\n",
    "                N += 1\n",
    "                question_titles[question_id] = title\n",
    "                unique_words_in_doc = set()\n",
    "\n",
    "                # Filter out stop words and build term frequency\n",
    "                for word in words:\n",
    "                    if word not in stop_words:\n",
    "                        term_frequencies[question_id][word] += 1\n",
    "                        unique_words_in_doc.add(word)\n",
    "\n",
    "                # Update document frequency\n",
    "                for word in unique_words_in_doc:\n",
    "                    df[word] += 1\n",
    "\n",
    "    # Precompute IDF values\n",
    "    idf = {word: math.log(N / (1 + freq)) for word, freq in df.items()}  # Add 1 to avoid division by zero\n",
    "\n",
    "    # Precompute TF-IDF vectors for all documents\n",
    "    for question_id, term_freqs in term_frequencies.items():\n",
    "        total_terms = sum(term_freqs.values())\n",
    "        tfidf_documents[question_id] = {\n",
    "            word: (freq / total_terms) * idf[word] for word, freq in term_freqs.items()\n",
    "        }\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(tf_query, tf_candidate):\n",
    "    dot_product = 0\n",
    "    norm_query = 0\n",
    "    norm_candidate = 0\n",
    "\n",
    "    # Calculate cosine similarity for TF-IDF\n",
    "    for word in tf_query:\n",
    "        if word in tf_candidate:\n",
    "            dot_product += tf_query[word] * tf_candidate[word]\n",
    "\n",
    "    # Calculate norms for TF-IDF vectors\n",
    "    for value in tf_query.values():\n",
    "        norm_query += value * value\n",
    "    for value in tf_candidate.values():\n",
    "        norm_candidate += value * value\n",
    "\n",
    "    # Final cosine similarity for TF-IDF\n",
    "    if norm_query == 0 or norm_candidate == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "\n",
    "    return dot_product / (norm_query**0.5 * norm_candidate**0.5)\n",
    "\n",
    "def process_query(query_question, answers_file, display_tfidf=False, display_similarity=False, display_answer = False):\n",
    "    query_terms = query_question.lower().split()\n",
    "    query_term_frequency = defaultdict(int)\n",
    "\n",
    "    # Calculate query term frequency\n",
    "    for term in query_terms:\n",
    "        if term not in stop_words:\n",
    "            query_term_frequency[term] += 1\n",
    "            \n",
    "\n",
    "    # Normalize query term frequencies and calculate query TF-IDF\n",
    "    total_query_terms = sum(query_term_frequency.values())\n",
    "    tf_query = {\n",
    "        term: (freq / total_query_terms) * idf.get(term, 0)\n",
    "        for term, freq in query_term_frequency.items()\n",
    "    }\n",
    "\n",
    "    # Compute cosine similarity with precomputed TF-IDF vectors\n",
    "    similarities = {}s\n",
    "    for question_id, tfidf in tfidf_documents.items():\n",
    "        # Compute similarity using only TF-IDF\n",
    "        similarity_score = cosine_similarity(tf_query, tfidf)\n",
    "        similarities[question_id] = similarity_score\n",
    "\n",
    "    # Get the top 10 question IDs based on similarity\n",
    "    top_indices = sorted(similarities, key=similarities.get, reverse=True)[:10]\n",
    "\n",
    "    print(\"Top questions and their corresponding answers:\")\n",
    "    with open(answers_file, 'r', encoding='latin-1') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        answer_dict = {}\n",
    "        for row in reader:\n",
    "            answer_question_id = row[3]\n",
    "            answer_content = row[5] \n",
    "            answer_dict[answer_question_id] = answer_content\n",
    "\n",
    "    # Display the top questions with their TF-IDF scores and cosine similarity\n",
    "    for rank, question_id in enumerate(top_indices, start=1):\n",
    "        print(f\"\\nRank: {rank}\")\n",
    "        print(f\"Question ID: {question_id}\")\n",
    "        print(f\"Title: {question_titles[question_id]}\")\n",
    "\n",
    "        if display_tfidf:\n",
    "            print(\"TF-IDF Scores for this question:\")\n",
    "            tfidf_scores = tfidf_documents[question_id]\n",
    "            for term, score in sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True):\n",
    "                print(f\"  {term}: {score:.4f}\")\n",
    "\n",
    "        if display_similarity:\n",
    "            print(f\"Cosine Similarity: {similarities[question_id]:.4f}\")\n",
    "\n",
    "        if question_id in answer_dict and display_answer:\n",
    "            print(f\"Answer: {answer_dict[question_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac75aee1-554d-48ed-9b01-3adf2614ca5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprecompute_tfidf(questions_file)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "precompute_tfidf(questions_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d885ca7e-9513-4f62-b802-26d8a67fd29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'R:\\Study\\5th Sem\\Information Retrieval\\IR PROJECT\\Saved Files\\TF-IDF\\idf.pkl', 'wb') as f:\n",
    "#     pickle.dump(idf, f)\n",
    "# with open(r'R:\\Study\\5th Sem\\Information Retrieval\\IR PROJECT\\Saved Files\\TF-IDF\\tfidf_documents.pkl', 'wb') as f:\n",
    "#     pickle.dump(tfidf_documents, f)\n",
    "# with open(r'R:\\Study\\5th Sem\\Information Retrieval\\IR PROJECT\\Saved Files\\TF-IDF\\question_titles.pkl', 'wb') as f:\n",
    "#     pickle.dump(question_titles, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b647b414-3e1b-4dd1-956d-0ee223950691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top questions and their corresponding answers:\n",
      "\n",
      "Rank: 1\n",
      "Question ID: 23682170\n",
      "Title: using for loops\n",
      "Cosine Similarity: 0.6093\n",
      "\n",
      "Rank: 2\n",
      "Question ID: 2874270\n",
      "Title: how to write this snippet in python?\n",
      "Cosine Similarity: 0.5354\n",
      "\n",
      "Rank: 3\n",
      "Question ID: 9864520\n",
      "Title: perl - regex how to write this in python?\n",
      "Cosine Similarity: 0.5319\n",
      "\n",
      "Rank: 4\n",
      "Question ID: 29885220\n",
      "Title: using objects in for of loops\n",
      "Cosine Similarity: 0.5071\n",
      "\n",
      "Rank: 5\n",
      "Question ID: 12222410\n",
      "Title: arrays and for loops\n",
      "Cosine Similarity: 0.5002\n",
      "\n",
      "Rank: 6\n",
      "Question ID: 13022350\n",
      "Title: for loops and arrays\n",
      "Cosine Similarity: 0.5002\n",
      "\n",
      "Rank: 7\n",
      "Question ID: 10745670\n",
      "Title: i am not sure how to use for loops to returns number of entries in table that are unique with python?\n",
      "Cosine Similarity: 0.4755\n",
      "\n",
      "Rank: 8\n",
      "Question ID: 7936410\n",
      "Title: loops and arrays in php\n",
      "Cosine Similarity: 0.4658\n",
      "\n",
      "Rank: 9\n",
      "Question ID: 1349900\n",
      "Title: multiple php while loops using the same query\n",
      "Cosine Similarity: 0.4565\n",
      "\n",
      "Rank: 10\n",
      "Question ID: 18069380\n",
      "Title: write bitarray to file in python?\n",
      "Cosine Similarity: 0.4507\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query_question = \"how to write for loops in python?\"\n",
    "# process_query(query_question, answers_file, display_tfidf=True, display_similarity=True, display_answer = False)\n",
    "process_query(query_question, answers_file, display_tfidf=False, display_similarity=True, display_answer = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34683059-dad8-471f-ad74-241005904649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top questions and their corresponding answers:\n",
      "\n",
      "Rank: 1\n",
      "Question ID: 9838900\n",
      "Title: java file io exceptions\n",
      "Cosine Similarity: 0.6940\n",
      "\n",
      "Rank: 2\n",
      "Question ID: 25099640\n",
      "Title: non-blocking io vs async io and implementation in java\n",
      "Cosine Similarity: 0.6904\n",
      "\n",
      "Rank: 3\n",
      "Question ID: 5467890\n",
      "Title: io in where clause\n",
      "Cosine Similarity: 0.6455\n",
      "\n",
      "Rank: 4\n",
      "Question ID: 8100280\n",
      "Title: java character io between java executions\n",
      "Cosine Similarity: 0.6105\n",
      "\n",
      "Rank: 5\n",
      "Question ID: 39811070\n",
      "Title: c - file io read and write error\n",
      "Cosine Similarity: 0.6024\n",
      "\n",
      "Rank: 6\n",
      "Question ID: 4950390\n",
      "Title: asp.net file io account\n",
      "Cosine Similarity: 0.5640\n",
      "\n",
      "Rank: 7\n",
      "Question ID: 17884730\n",
      "Title: io operation failure\n",
      "Cosine Similarity: 0.5324\n",
      "Answer: <p>I am just adding kwatford`s comment as answer in here.  What you need to change is </p>\n",
      "\n",
      "<pre><code>filename = os.path.join(getMediaPath(),aFile)\n",
      "\n",
      "newfile = os.path.join(getMediaPath() , 'happyEdited.txt')\n",
      "</code></pre>\n",
      "\n",
      "\n",
      "Rank: 8\n",
      "Question ID: 22165860\n",
      "Title: blocking io in nodejs\n",
      "Cosine Similarity: 0.5204\n",
      "Answer: <blockquote>\n",
      "  <p>Here is my question, is there a design issue in the solution I just mentioned perhaps about nested callbacks.</p>\n",
      "</blockquote>\n",
      "\n",
      "<p>No, your solution is perfectly fine.</p>\n",
      "\n",
      "<blockquote>\n",
      "  <p>Second is there any method in which I can make NodeJs IO as Blocking from NON-Blocking?</p>\n",
      "</blockquote>\n",
      "\n",
      "<p>Yes. You can write your own c++ extension to provide blocking calls.</p>\n",
      "\n",
      "\n",
      "Rank: 9\n",
      "Question ID: 8852500\n",
      "Title: passing values of io process to another class in real time in java\n",
      "Cosine Similarity: 0.5100\n",
      "\n",
      "Rank: 10\n",
      "Question ID: 33918950\n",
      "Title: socket io won't function on openshift\n",
      "Cosine Similarity: 0.4886\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query_question = \"java io error\"\n",
    "# process_query(query_question, answers_file, display_tfidf=True, display_similarity=True, display_answer = False)\n",
    "process_query(query_question, answers_file, display_tfidf=False, display_similarity=True, display_answer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5880aa40-255f-4bc0-bbee-ab7b2e87ad5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top questions and their corresponding answers:\n",
      "\n",
      "Rank: 1\n",
      "Question ID: 20989620\n",
      "Title: content search engine\n",
      "\n",
      "Rank: 2\n",
      "Question ID: 6915280\n",
      "Title: php search engine for mysql\n",
      "\n",
      "Rank: 3\n",
      "Question ID: 35133590\n",
      "Title: building sql query for search string\n",
      "Answer: <p>Setting the SqlCommand.CommandText after you have passed it to SqlDataAdapter doesn't change the text stored in the adapter, moving the setting of the SqlCommand.Commandtext before the creation of the adapter seems to be a good fix....</p>\n",
      "\n",
      "<pre><code>cmd.CommandText = Sql\n",
      "Dim adapter = New SqlDataAdapter(cmd.CommandText, con.ConnectionString)\n",
      "Dim dt As New DataTable()\n",
      "adapter.Fill(dt)\n",
      "</code></pre>\n",
      "\n",
      "<p>but, wait, you still have a problem. When you pass a string (as the CommandText) to the adapter constructor it builds internally another SqlCommand using that string. This internal command has its Parameter collection empty. It doesn't know anything of the parameter collection created externally by your code.<br>\n",
      "So, the real fix is to create the adapter using directly the SqlCommand prepared by your code.</p>\n",
      "\n",
      "<pre><code>cmd.CommandText = Sql\n",
      "cmd.Connection = con\n",
      "\n",
      "' Pass the SqlCommand, it will be used to be the SelectCommand of the adapter'\n",
      "Dim adapter = New SqlDataAdapter(cmd)\n",
      "Dim dt As New DataTable()\n",
      "' Run the SelectCommand'\n",
      "adapter.Fill(dt)\n",
      "</code></pre>\n",
      "\n",
      "\n",
      "Rank: 4\n",
      "Question ID: 5612580\n",
      "Title: page source search engine\n",
      "\n",
      "Rank: 5\n",
      "Question ID: 19970840\n",
      "Title: creating a custom search engine with asp.net\n",
      "\n",
      "Rank: 6\n",
      "Question ID: 17222810\n",
      "Title: complex google app engine search\n",
      "Answer: <p>1) The SearchService basically gives you an API to perform the sorts of things you can't using the datastore. If you could do them on the datastore, you wouldn't really need the SearchService. While not a very satisfactory answer, many of the common operations you might do with a traditional RDBMS were not really even possible before the Search API was available.</p>\n",
      "\n",
      "<p>2) is a bit harder. Currently the search api doesn't handle failure conditions very well, usually you'll get a SearchServiceException without a meaningful message. The team seem to have been improving this over the last year or so, although fixes in this space seem to have been coming very slowly. \n",
      "From the tickets I've raised, failures are usually a result of queries running too long. This is usually represented as queries that are too complex. You can actually tune queries quite a lot with combinations of the query string as well as the parameters you apply to your search request. The downside is that its all totally black box, I haven't seen any guides or tools on optimising queries. When they fail, they just fail.</p>\n",
      "\n",
      "<p>The AppEngine search api is designed to solve the problems you describe, whether in your case it does may be hard to determine. You could set up some sample queries and deploy to a test environment to see if it even basically works for your typical set of data. I would expect that it will work fine for the example you gave. I have successfully been running similar searches in large scale production environments.</p>\n",
      "\n",
      "\n",
      "Rank: 7\n",
      "Question ID: 14836410\n",
      "Title: google app engine java text search api doesn't work same as python in string search\n",
      "Answer: <p>Unfortunately this is still not possible given the API.</p>\n",
      "\n",
      "<p>See these posts for more information: </p>\n",
      "\n",
      "<ul>\n",
      "<li><a href=\"http://stackoverflow.com/questions/12899083/partial-matching-gae-search-api/13171181\">Partial matching GAE search API</a></li>\n",
      "<li><a href=\"http://stackoverflow.com/questions/10523987/gae-full-text-search-api-phrase-matching/\">GAE Full Text Search API phrase matching</a></li>\n",
      "</ul>\n",
      "\n",
      "<p>This is also logged as an issue/defect here:</p>\n",
      "\n",
      "<ul>\n",
      "<li><a href=\"http://code.google.com/p/googleappengine/issues/detail?id=7689\" rel=\"nofollow\">http://code.google.com/p/googleappengine/issues/detail?id=7689</a></li>\n",
      "</ul>\n",
      "\n",
      "\n",
      "Rank: 8\n",
      "Question ID: 33994730\n",
      "Title: create custom search engine in google using xml file\n",
      "\n",
      "Rank: 9\n",
      "Question ID: 33953250\n",
      "Title: search engine based on google spreadsheet\n",
      "\n",
      "Rank: 10\n",
      "Question ID: 18439620\n",
      "Title: how to use a rails engine in a rails engine\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "query_question = \"building a search engine\"\n",
    "# process_query(query_question, answers_file, display_tfidf=True, display_similarity=True, display_answer = False)\n",
    "process_query(query_question, answers_file, display_tfidf=False, display_similarity=False, display_answer = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49eecb-beeb-4660-b6b7-ec8dd9bb0b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd35712-b5a2-4959-a327-30543a1250e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
